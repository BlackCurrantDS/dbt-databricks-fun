{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb924e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook: Download artifacts for latest run of a given task of THIS job\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# PARAMETERS\n",
    "# ----------\n",
    "dbutils.widgets.text(\"TASK_KEY\", \"\")               # task_key (task name) within this job\n",
    "dbutils.widgets.text(\"SOURCE_PATH\", \"\")            # subfolder inside artifacts, empty for root\n",
    "dbutils.widgets.text(\"DEST_PATH\", \"\")   # local/DBFS dest on this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb9f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_KEY = dbutils.widgets.get(\"TASK_KEY\").strip()\n",
    "SOURCE_PATH = dbutils.widgets.get(\"SOURCE_PATH\").strip()\n",
    "DEST_PATH = dbutils.widgets.get(\"DEST_PATH\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Databricks SDK\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get JOB_ID of *this* notebook's job\n",
    "# --------------------------------------\n",
    "ctx_json_str = dbutils.notebook.entry_point.getDbutils().notebook().getContext().safeToJson()\n",
    "print(type(ctx_json_str))\n",
    "ctx_dict = json.loads(ctx_json_str)\n",
    "print(ctx_dict)\n",
    "\n",
    "# 2. Get job_id from the dict\n",
    "JOB_ID = ctx_dict['attributes'].get(\"multitaskParentRunId\")\n",
    "\n",
    "print(f\"Current job id (JOB_ID) = {JOB_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ca3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_args = dbutils.notebook.entry_point.getCurrentBindings()\n",
    "\n",
    "print(all_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5594d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client using the job's identity (service principal / compute identity)\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Get run info (Jobs API 2.1)\n",
    "run = w.jobs.get_run(run_id=int(JOB_ID))\n",
    "\n",
    "# `run` is a JobsGetRunResponse; `run.tasks` is a list of RunTask instances\n",
    "tasks = run.tasks or []\n",
    "\n",
    "# Pretty-print equivalent to `jq .tasks`\n",
    "print(json.dumps([t.as_dict() for t in tasks], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ea6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_run_id = next(\n",
    "    (t.run_id for t in tasks if t.task_key == \"dbt\"),\n",
    "    None,  # fallback if not found\n",
    ")\n",
    "\n",
    "print(dbt_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35db458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the details of the taks\n",
    "dbt_task_out = w.jobs.get_run_output(run_id=dbt_run_id)\n",
    "print(dbt_task_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eeff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_artifact_url = None\n",
    "if dbt_task_out.dbt_output is not None:\n",
    "    dbt_artifact_url = dbt_task_out.dbt_output.artifacts_link\n",
    "\n",
    "print(\"DBT_ARTIFACT_URL:\", dbt_artifact_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a85617",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"/tmp/artifact.tar.gz\"\n",
    "volume_path = \"/Volumes/workspace/default/dbt_artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec51425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should work if auth is valid\n",
    "me = w.current_user.me()\n",
    "print(\"Workspace user:\", me.user_name or me.display_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f1f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": f\"Bearer {w.config.token}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f6f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c99660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Ensure the destination directory exists within the Volume\n",
    "os.makedirs(os.path.dirname(volume_path), exist_ok=True)\n",
    "\n",
    "print(f\"Streaming artifact directly to Volume: {volume_path}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec39bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_file = os.path.join(volume_path, \"artifact.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 4. Stream the download to avoid loading large files into RAM\n",
    "    with requests.get(dbt_artifact_url, headers=headers, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest_file, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 1024): # 1MB chunks\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    \n",
    "    print(\"Transfer successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to store artifact in Volume: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc0423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "print(f\"Opening archive: {dest_file}\")\n",
    "try:\n",
    "    # mode \"r:*\" lets tarfile auto-detect gzip/bzip2/xz/plain tar\n",
    "    with tarfile.open(dest_file, mode=\"r:*\") as tar_ref:\n",
    "        # 1. List all members to find index.html (could be in a subfolder)\n",
    "        all_members = tar_ref.getmembers()\n",
    "        target_member = next(\n",
    "            (m for m in all_members if m.name.endswith(\"index.html\")),\n",
    "            None\n",
    "        )\n",
    "\n",
    "        if target_member:\n",
    "            print(f\"Found it! Extracting {target_member.name}...\")\n",
    "\n",
    "            # 2. Extract only that one file\n",
    "            tar_ref.extract(target_member, path=volume_path)\n",
    "\n",
    "            # 3. Move/rename to a clean path if nested (e.g. target/index.html)\n",
    "            old_path = os.path.join(volume_path, target_member.name)\n",
    "            new_path = os.path.join(volume_path, \"index.html\")\n",
    "\n",
    "            # Ensure the parent directory exists for old_path and new_path\n",
    "            os.makedirs(os.path.dirname(old_path), exist_ok=True)\n",
    "            os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "\n",
    "            if old_path != new_path:\n",
    "                os.replace(old_path, new_path)\n",
    "                print(f\"Cleaned up path. File is now at: {new_path}\")\n",
    "        else:\n",
    "            print(\"Error: Could not find index.html in the tar archive.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The tar file wasn't found. Check your download step.\")\n",
    "except tarfile.ReadError:\n",
    "    print(\"Error: The file is not a valid tar/tar.gz archive.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd8bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "print(f\"Opening archive: {dest_file}\")\n",
    "try:\n",
    "    # mode \"r:*\" lets tarfile auto-detect gzip/bzip2/xz/plain tar\n",
    "    with tarfile.open(dest_file, mode=\"r:*\") as tar_ref:\n",
    "        all_members = tar_ref.getmembers()\n",
    "\n",
    "        # 1. Find index.html (docs)\n",
    "        index_member = next(\n",
    "            (m for m in all_members if m.name.endswith(\"index.html\")),\n",
    "            None\n",
    "        )\n",
    "\n",
    "        # 2. Find dbt log files (usually logs/*.log)\n",
    "        log_members = [\n",
    "            m for m in all_members\n",
    "            if m.name.startswith(\"logs/\") and m.name.endswith(\".log\")\n",
    "        ]\n",
    "\n",
    "        # --- Handle index.html ---\n",
    "        if index_member:\n",
    "            print(f\"Found index.html at {index_member.name}. Extracting...\")\n",
    "            tar_ref.extract(index_member, path=volume_path)\n",
    "\n",
    "            old_index_path = os.path.join(volume_path, index_member.name)\n",
    "            new_index_path = os.path.join(volume_path, \"index.html\")\n",
    "\n",
    "            os.makedirs(os.path.dirname(old_index_path), exist_ok=True)\n",
    "            os.makedirs(os.path.dirname(new_index_path), exist_ok=True)\n",
    "\n",
    "            if old_index_path != new_index_path:\n",
    "                os.replace(old_index_path, new_index_path)\n",
    "                print(f\"Docs index is now at: {new_index_path}\")\n",
    "        else:\n",
    "            print(\"Warning: Could not find index.html in the tar archive.\")\n",
    "\n",
    "        # --- Handle dbt logs ---\n",
    "        if log_members:\n",
    "            logs_target_dir = os.path.join(volume_path, \"logs\")\n",
    "            os.makedirs(logs_target_dir, exist_ok=True)\n",
    "\n",
    "            for m in log_members:\n",
    "                print(f\"Extracting log file: {m.name}\")\n",
    "                tar_ref.extract(m, path=volume_path)\n",
    "\n",
    "                # m.name is like \"logs/dbt.log\" or \"logs/<timestamp>.log\"\n",
    "                old_log_path = os.path.join(volume_path, m.name)\n",
    "                log_filename = os.path.basename(m.name)\n",
    "                new_log_path = os.path.join(logs_target_dir, log_filename)\n",
    "\n",
    "                if old_log_path != new_log_path:\n",
    "                    os.replace(old_log_path, new_log_path)\n",
    "\n",
    "            print(f\"Extracted {len(log_members)} log file(s) into {logs_target_dir}\")\n",
    "        else:\n",
    "            print(\"Warning: No dbt log files (logs/*.log) found in the tar archive.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The tar file wasn't found. Check your download step.\")\n",
    "except tarfile.ReadError:\n",
    "    print(\"Error: The file is not a valid tar/tar.gz archive.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
